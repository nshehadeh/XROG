{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLL8YfcLGxZM",
        "outputId": "ff91be93-744f-4623-93ba-4c04468b84fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'gdrive/My Drive/CPS Final Proj'\n",
            "/content/gdrive/My Drive/CPS Final Proj\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/CPS Final Proj"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "uzozRH7wG2kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.chdir(\"data/\")\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "o10lU46iHCqV",
        "outputId": "5af2dcce-657f-45ee-fdac-d51ef11bec22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-815fcc6d19cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('shields.csv')\n",
        "shields = np.asarray(df)"
      ],
      "metadata": {
        "id": "gtjbjpCwHDUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shields.shape)"
      ],
      "metadata": {
        "id": "S7d3r60UIJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import plot\n",
        "\n",
        "\n",
        "\n",
        "def read_csv(file_path):\n",
        "    data = pd.read_csv(file_path, header=None, names=['sequence_num', 'x', 'y', 'z'])\n",
        "    return data\n",
        "\n",
        "def plot_point_cloud_matplotlib(data, sequence_num):\n",
        "    sequence_data = data[data['sequence_num'] == sequence_num]\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(sequence_data['x'], sequence_data['y'], sequence_data['z'], s=50, alpha=0.6, edgecolors='w', depthshade=True)\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    plt.show()\n",
        "\n",
        "def plot_point_cloud_plotly(data, sequence_num):\n",
        "    sequence_data = data[data['sequence_num'] == sequence_num]\n",
        "    fig = px.scatter_3d(sequence_data, x='x', y='y', z='z', opacity=0.6)\n",
        "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
        "    fig.show()\n",
        "\n",
        "def plot_point_clouds_by_class_plotly(data_array, labels, target_class, aligned):\n",
        "    target_indices = np.where(labels == target_class)[0]\n",
        "    data = []\n",
        "\n",
        "    for idx in target_indices:\n",
        "        point_cloud = data_array[idx]\n",
        "        x, y, z = point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2]\n",
        "        scatter = go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=2, opacity=0.6))\n",
        "        data.append(scatter)\n",
        "\n",
        "    layout = go.Layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
        "    fig = go.Figure(data=data, layout=layout)\n",
        "    \n",
        "    # Save the figure to an HTML file\n",
        "    if(aligned):\n",
        "      path = \"3d_plot_class_aligned_\" + str(target_class) + \".html\"\n",
        "    else:\n",
        "      path = \"3d_plot_class_\" + str(target_class) + \".html\"\n",
        "    plot(fig, filename=path, auto_open=False)\n",
        "    \n",
        "    # Optionally return the figure object\n",
        "    return fig\n",
        "\n",
        "def plot_point_cloud_plotly_numpy(data, sequence_num):\n",
        "    sequence_data = data[sequence_num]\n",
        "    x, y, z = sequence_data[:, 0], sequence_data[:, 1], sequence_data[:, 2]\n",
        "    fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(opacity=0.6))])\n",
        "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'))\n",
        "    fig.show()\n",
        "\n",
        "def convert_to_numpy_array(data):\n",
        "    sequences = []\n",
        "    for seq_num in data['sequence_num'].unique():\n",
        "        sequence_data = data[data['sequence_num'] == seq_num]\n",
        "        sequence_matrix = sequence_data[['x', 'y', 'z']].to_numpy()\n",
        "        sequences.append(sequence_matrix)\n",
        "    return np.array(sequences, dtype=object)\n",
        "\n",
        "def average_sequence_length(arrays):\n",
        "    lengths = [arr.shape[0] for arr in arrays]\n",
        "    return int(np.mean(lengths))\n",
        "\n",
        "def std_frames_per_sequence(arr):\n",
        "    frame_counts = np.array([len(sequence) for sequence in arr])\n",
        "    return np.std(frame_counts, ddof=1)\n",
        "\n",
        "def plot_frame_sizes(arr):\n",
        "    num_sequences = len(arr)\n",
        "    frame_counts = [len(sequence) for sequence in arr]\n",
        "    sequence_numbers = range(num_sequences)\n",
        "\n",
        "    plt.bar(sequence_numbers, frame_counts)\n",
        "    plt.xlabel('Sequence Number')\n",
        "    plt.ylabel('Number of Frames')\n",
        "    plt.title('Frame Count per Sequence')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EmuEOE0hIKtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shields_data = read_csv(\"shields.csv\")\n",
        "#plot_point_cloud_plotly(shields_data, sequence_num=4)\n",
        "shields_arr = convert_to_numpy_array(shields_data)\n",
        "plot_frame_sizes(shields_arr)\n",
        "avg_frames = average_sequence_length(shields_arr)\n",
        "print(\"Average number of frames per sequence:\", avg_frames)"
      ],
      "metadata": {
        "id": "8yVXXNqsL5Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stars_data = read_csv(\"stars.csv\")\n",
        "#plot_point_cloud_plotly(stars_data, sequence_num=19)\n",
        "stars_arr = convert_to_numpy_array(stars_data)\n",
        "plot_frame_sizes(stars_arr)\n",
        "avg_frames = average_sequence_length(stars_arr)\n",
        "print(\"Average number of frames per sequence:\", avg_frames)"
      ],
      "metadata": {
        "id": "m0mb8kfAQZsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swords_data = read_csv(\"swords.csv\")\n",
        "#plot_point_cloud_plotly(swords_data, sequence_num=19)\n",
        "swords_arr = convert_to_numpy_array(swords_data)\n",
        "plot_frame_sizes(swords_arr)\n",
        "avg_frames = average_sequence_length(swords_arr)\n",
        "print(\"Average number of frames per sequence:\", avg_frames)"
      ],
      "metadata": {
        "id": "p8Q-j1mpR0jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STD shields: \", std_frames_per_sequence(shields_arr), \" NP \", np.std(np.array([len(sequence) for sequence in shields_arr])))\n",
        "print(\"STD swords: \", std_frames_per_sequence(swords_arr))\n",
        "print(\"STD stars: \", std_frames_per_sequence(stars_arr))"
      ],
      "metadata": {
        "id": "vQr2JfDgSCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_sequences(arr):\n",
        "    frame_counts = np.array([len(sequence) for sequence in arr])\n",
        "    avg_frames = np.mean(frame_counts)\n",
        "    std_dev = np.std(frame_counts, ddof=1)\n",
        "\n",
        "    threshold = avg_frames - 2*std_dev\n",
        "    filtered_arr = np.array([sequence for sequence in arr if len(sequence) >= threshold], dtype=object)\n",
        "\n",
        "    return filtered_arr\n",
        "\n",
        "swords_arr_filtered = remove_short_sequences(swords_arr)\n",
        "shields_arr_filtered = remove_short_sequences(shields_arr)\n",
        "stars_arr_filtered = remove_short_sequences(stars_arr)\n",
        "\n"
      ],
      "metadata": {
        "id": "iNt4gCLoULqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"STD swords: \", std_frames_per_sequence(swords_arr_filtered), \" NP \", np.std(np.array([len(sequence) for sequence in swords_arr_filtered])))\n",
        "print(\"Original swords sequence: \", swords_arr.size, \" |  New swords sequence: \", swords_arr_filtered.size)\n",
        "print(\"Original shields sequence: \", shields_arr.size, \" |  New shields sequence: \", shields_arr_filtered.size)\n",
        "print(\"Original stars sequence: \", stars_arr.size, \" |  New stars sequence: \", stars_arr_filtered.size)\n",
        "\n",
        "min_size = min(len(swords_arr_filtered), len(shields_arr_filtered), len(stars_arr_filtered))\n",
        "\n",
        "# resize each array to the size of the smallest array\n",
        "swords_arr_resized = np.resize(swords_arr_filtered, min_size)\n",
        "shields_arr_resized = np.resize(shields_arr_filtered, min_size)\n",
        "stars_arr_resized = np.resize(stars_arr_filtered, min_size)\n",
        "\n",
        "print(\"New size swords: \", swords_arr_resized.size)\n",
        "print(\"New size shields: \", shields_arr_resized.size)\n",
        "print(\"New size stars: \", stars_arr_resized.size)"
      ],
      "metadata": {
        "id": "j_06aOtjVwxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ran"
      ],
      "metadata": {
        "id": "G_YL_wXifVHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "def get_average_sequence_length(swords_arr, shields_arr, stars_arr):\n",
        "  total_length = 0\n",
        "  total_sequences = 0\n",
        "\n",
        "  for gesture_arr in [swords_arr, shields_arr, stars_arr]:\n",
        "      for sequence in gesture_arr:\n",
        "          total_length += sequence.shape[0]\n",
        "          total_sequences += 1\n",
        "\n",
        "  return int(total_length / total_sequences)\n",
        "\n",
        "def nearest_neighbor_resampling(sequence, target_length):\n",
        "  current_length = sequence.shape[0]\n",
        "  indices = np.round(np.linspace(0, current_length - 1, target_length)).astype(int)\n",
        "  return sequence[indices]\n",
        "\n",
        "def furthest_point_sampling(sequence, target_length):\n",
        "  current_length = sequence.shape[0]\n",
        "  remaining_indices = set(range(current_length))\n",
        "  sampled_indices = [0]\n",
        "  remaining_indices.remove(0)\n",
        "\n",
        "  while len(sampled_indices) < target_length:\n",
        "      distances = cdist(sequence[sampled_indices[-1]].reshape(1, -1), sequence[list(remaining_indices)])\n",
        "      furthest_point = remaining_indices.pop(np.argmax(distances))\n",
        "      sampled_indices.append(furthest_point)\n",
        "\n",
        "  sampled_indices.sort()\n",
        "  return sequence[sampled_indices]\n",
        "  return array[np.array(sorted(selected_indices))]\n",
        "\n",
        "def random_sampling(sequence, target_length):\n",
        "  current_length = sequence.shape[0]\n",
        "  indices = np.random.choice(current_length, target_length, replace=False)\n",
        "  indices.sort()\n",
        "  return sequence[indices]\n",
        "\n",
        "def prepare_data(swords_arr, shields_arr, stars_arr, average_len):\n",
        "  average_sequence_length = average_len\n",
        "  total_samples = len(swords_arr) + len(shields_arr)+len(stars_arr)\n",
        "  input_data = np.zeros((total_samples, average_len, 3))\n",
        "  labels = np.zeros((total_samples))\n",
        "  i = 0\n",
        "  for gesture_arr, label in [(swords_arr, 0), (shields_arr, 1), (stars_arr, 2)]:\n",
        "    for sequence in gesture_arr:\n",
        "      current_length = sequence.shape[0]\n",
        "      if current_length < average_sequence_length:\n",
        "          resampled_sequence = nearest_neighbor_resampling(sequence, average_sequence_length)\n",
        "      elif current_length > average_sequence_length:\n",
        "          resampled_sequence = random_sampling(sequence, average_sequence_length)\n",
        "      else:\n",
        "          resampled_sequence = sequence\n",
        "      \n",
        "      input_data[i] = resampled_sequence\n",
        "      labels[i] = label\n",
        "      i = i + 1\n",
        "\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  return input_data, labels\n"
      ],
      "metadata": {
        "id": "tKWXd3pTHQ1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average sequence length\n",
        "avg_length = (average_sequence_length(swords_arr_resized) + average_sequence_length(stars_arr_resized) + average_sequence_length(shields_arr_resized))/3\n",
        "avg_length = int(avg_length)\n",
        "print(\"Average length for sampling: \", avg_length)\n",
        "print(\"Total number of samples: \", len(stars_arr_resized)+len(swords_arr_resized)+len(shields_arr_resized))\n",
        "# Prepare the data for input into an SVM model\n",
        "input_data, labels = prepare_data(swords_arr_resized, shields_arr_resized, stars_arr_resized, avg_length)\n",
        "print(\"Compiled data: \", np.shape(input_data))\n",
        "print(\"Compiled labels: \", np.shape(labels))"
      ],
      "metadata": {
        "id": "fJLGX1MuObAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_point_cloud_plotly_numpy(input_data, 45)"
      ],
      "metadata": {
        "id": "Cxn3IufHdPc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install open3d\n"
      ],
      "metadata": {
        "id": "YYtxr1M7dlhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "\n",
        "# ICP Alignment\n",
        "def preprocess_point_cloud(point_cloud):\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
        "\n",
        "    # Translate the centroid of the point cloud to the origin\n",
        "    centroid = np.mean(point_cloud, axis=0)\n",
        "    pcd.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) - centroid)\n",
        "\n",
        "    # Scale the point cloud to a fixed maximum size (e.g., 1 unit)\n",
        "    max_size = 1.0\n",
        "    distances = np.linalg.norm(np.asarray(pcd.points), axis=1)\n",
        "    max_distance = np.max(distances)\n",
        "    pcd.points = o3d.utility.Vector3dVector(np.asarray(pcd.points) * (max_size / max_distance))\n",
        "\n",
        "    return pcd\n",
        "def preprocess_point_cloud_np(point_cloud):\n",
        "    # Translate the centroid of the point cloud to the origin\n",
        "    centroid = np.mean(point_cloud, axis=0)\n",
        "    point_cloud = point_cloud - centroid\n",
        "\n",
        "    # Scale the point cloud to a fixed maximum size (e.g., 1 unit)\n",
        "    max_size = 1.0\n",
        "    distances = np.linalg.norm(point_cloud, axis=1)\n",
        "    max_distance = np.max(distances)\n",
        "    point_cloud = point_cloud * (max_size / max_distance)\n",
        "\n",
        "    return point_cloud\n",
        "def execute_global_registration(source, target, distance_threshold):\n",
        "    result = o3d.pipelines.registration.registration_icp(source, target, distance_threshold,\n",
        "                                                          np.identity(4),\n",
        "                                                          o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
        "                                                          o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=50))\n",
        "    return result\n",
        "\n",
        "def align_point_clouds(data_array, labels, distance_threshold=1.5):\n",
        "    unique_labels = np.unique(labels)\n",
        "    aligned_data_array = []\n",
        "    aligned_labels = []\n",
        "\n",
        "    for label in unique_labels:\n",
        "        class_indices = np.where(labels == label)[0]\n",
        "        base_sample = data_array[class_indices[0]]\n",
        "        print(\"here\", base_sample.shape)\n",
        "        base_pcd = preprocess_point_cloud_np(base_sample)\n",
        "        aligned_class_samples = [base_sample]\n",
        "\n",
        "        for idx in class_indices[1:]:\n",
        "            sample = data_array[idx]\n",
        "            sample_pcd = preprocess_point_cloud_np(sample)\n",
        "            aligned_class_samples.append(np.asarray(sample_pcd))\n",
        "\n",
        "        aligned_data_array.extend(aligned_class_samples)\n",
        "        aligned_labels.extend([label] * len(aligned_class_samples))\n",
        "\n",
        "    return np.array(aligned_data_array), np.array(aligned_labels)\n",
        "\n",
        "\n",
        "aligned_data_array, aligned_labels = align_point_clouds(input_data, labels)\n",
        "print(aligned_data_array.shape)"
      ],
      "metadata": {
        "id": "nKzRG-QGo4V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QF2ksDl4qBey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test alignment a couple random points\n",
        "\"\"\"\n",
        "random_numbers = np.random.randint(0, 88, 3)\n",
        "for num in random_numbers:\n",
        "  print(\"Original sample: \", num)\n",
        "  plot_point_cloud_plotly_numpy(input_data, num)\n",
        "  print(\"Label: \", labels[num], \" (swords_arr, 0), (shields_arr, 1), (stars_arr, 2)\")\n",
        "  print(\"Aligned sample: \", num)\n",
        "  plot_point_cloud_plotly_numpy(aligned_data_array, num)\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "euvyCXG2x83Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/gdrive/MyDrive/CPS Final Proj/plots\")\n",
        "plot_point_clouds_by_class_plotly(input_data,labels,1, False)"
      ],
      "metadata": {
        "id": "9GXfTN1nN-Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot_point_clouds_by_class_plotly(aligned_data_array,aligned_labels,1, True)\n"
      ],
      "metadata": {
        "id": "no5zVtMASwDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(aligned_data_array, aligned_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train size pre reshape: \", X_train.shape)\n",
        "sample = X_test[0]"
      ],
      "metadata": {
        "id": "Urt9Bqf4frE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation functions\n",
        "def translation(points, max_translation):\n",
        "    tx, ty, tz = np.random.uniform(-max_translation, max_translation, 3)\n",
        "    return points + np.array([tx, ty, tz])\n",
        "\n",
        "def rotation(points, max_angle):\n",
        "    angle_x, angle_y, angle_z = np.random.uniform(-max_angle, max_angle, 3)\n",
        "    rot_x = np.array([[1, 0, 0],\n",
        "                      [0, np.cos(angle_x), -np.sin(angle_x)],\n",
        "                      [0, np.sin(angle_x), np.cos(angle_x)]])\n",
        "    rot_y = np.array([[np.cos(angle_y), 0, np.sin(angle_y)],\n",
        "                      [0, 1, 0],\n",
        "                      [-np.sin(angle_y), 0, np.cos(angle_y)]])\n",
        "    rot_z = np.array([[np.cos(angle_z), -np.sin(angle_z), 0],\n",
        "                      [np.sin(angle_z), np.cos(angle_z), 0],\n",
        "                      [0, 0, 1]])\n",
        "    rotation_matrix = rot_x @ rot_y @ rot_z\n",
        "    return points @ rotation_matrix\n",
        "\n",
        "def scaling(points, min_scale, max_scale):\n",
        "    scale_factor = np.random.uniform(min_scale, max_scale)\n",
        "    return points * scale_factor\n",
        "\n",
        "def add_noise(points, noise_std_dev):\n",
        "    noise = np.random.normal(0, noise_std_dev, points.shape)\n",
        "    return points + noise\n",
        "\n",
        "def apply_augmentations(points, num_augmentations, **kwargs):\n",
        "    augmented_data = []\n",
        "\n",
        "    for _ in range(num_augmentations):\n",
        "        new_points = points.copy()\n",
        "        new_points = translation(new_points, kwargs.get(\"max_translation\", 0.1))\n",
        "        new_points = rotation(new_points, kwargs.get(\"max_angle\", np.radians(10)))\n",
        "        new_points = scaling(new_points, kwargs.get(\"min_scale\", 0.9), kwargs.get(\"max_scale\", 1.1))\n",
        "        new_points = add_noise(new_points, kwargs.get(\"noise_std_dev\", 0.01))\n",
        "        augmented_data.append(new_points)\n",
        "\n",
        "    return np.array(augmented_data)\n",
        "\n",
        "# Augment data\n",
        "max_angle = np.radians(15)\n",
        "min_scale = 0.8\n",
        "max_scale = 1.2\n",
        "noise_std_dev = 0.01\n",
        "# Apply augmentations\n",
        "num_augmentations = 10\n",
        "augmented_data = []\n",
        "for sample in X_train:\n",
        "    sample_3d = sample.reshape(-1, 3)\n",
        "    new_samples = apply_augmentations(sample_3d, num_augmentations=num_augmentations, max_angle=max_angle, min_scale=min_scale, max_scale=max_scale, noise_std_dev=noise_std_dev)\n",
        "    augmented_data.extend(new_samples.reshape(-1, 459))\n",
        "augmented_data = np.array(augmented_data)\n",
        "\n",
        "# Create y_train_augmented and concatenate with y_train\n",
        "y_train_augmented = np.repeat(y_train, num_augmentations)\n",
        "y_train_combined = np.hstack((y_train, y_train_augmented))\n",
        "\n",
        "# Combine X_train and augmented_data\n",
        "X_train_augmented = np.vstack((X_train.reshape(X_train.shape[0], -1), augmented_data))\n",
        "print(\"Augmented data size: \", X_train_augmented.shape)\n",
        "print(\"Augmented label size: \", y_train_combined.shape)"
      ],
      "metadata": {
        "id": "punyXw8ed37z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Flatten the 3D array into a 2D array of shape (num_samples, num_frames * 3)\n",
        "#X_train = augmented_data.reshape((X_train.shape[0], -1))\n",
        "# Augment data?\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Train an SVM model\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "# Perform 5-fold cross-validation on the training data\n",
        "k = 5\n",
        "scores = cross_val_score(svm_model, X_train_augmented, y_train_combined, cv=k)\n",
        "\n",
        "# Calculate the average score\n",
        "average_score = np.mean(scores)\n",
        "\n",
        "print(f\"Average cross-validation score: {average_score:.2f}\")\n",
        "\n",
        "# Train the model using the entire training set\n",
        "svm_model.fit(X_train_augmented, y_train_combined)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "# Evaluate the model on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix and the classification report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = svm_model.score(X_test, y_test)\n",
        "\n",
        "# Print the confusion matrix, the classification report, and the accuracy\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"Classification report:\\n\", cr)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "zcoooNMXddp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "import pickle\n",
        "\n",
        "# Assume 'svm_model' is your trained SVM model\n",
        "with open('svm_model.pkl', 'wb') as file:\n",
        "    pickle.dump(svm, file)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('svm_model.pkl')"
      ],
      "metadata": {
        "id": "6VGIeuo5hmhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[0].shape)\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "id": "A3brCbkciCzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def srandom_sampling(sequence, target_length):\n",
        "    current_length = sequence.shape[0]\n",
        "    indices = np.random.choice(current_length, target_length, replace=False)\n",
        "    indices.sort()I\n",
        "    return sequence[indices]\n",
        "def snearest_neighbor_resampling(sequence, target_length):\n",
        "    current_length = sequence.shape[0]\n",
        "    indices = np.round(np.linspace(0, current_length - 1, target_length)).astype(int)\n",
        "    return sequence[indices]\n",
        "def sprepare_data(arr, average_len):\n",
        "    average_sequence_length = average_len\n",
        "    i = 0\n",
        "    current_length = arr.shape[0]\n",
        "    if current_length < average_sequence_length:\n",
        "        resampled_sequence = snearest_neighbor_resampling(arr, average_sequence_length)\n",
        "    elif current_length > average_sequence_length:\n",
        "        resampled_sequence = srandom_sampling(arr, average_sequence_length)\n",
        "    else:\n",
        "        resampled_sequence = arr\n",
        "    return resampled_sequence\n",
        "\n",
        "def spreprocess_point_cloud_np(point_cloud):\n",
        "    # Translate the centroid of the point cloud to the origin\n",
        "    centroid = np.mean(point_cloud, axis=0)\n",
        "    point_cloud = point_cloud - centroid\n",
        "\n",
        "    # Scale the point cloud to a fixed maximum size (e.g., 1 unit)\n",
        "    max_size = 1.0\n",
        "    distances = np.linalg.norm(point_cloud, axis=1)\n",
        "    max_distance = np.max(distances)\n",
        "    point_cloud = point_cloud * (max_size / max_distance)\n",
        "\n",
        "    return point_cloud\n",
        "my_list = np.random.rand(534)\n",
        "\n",
        "inp = np.array(my_list)\n",
        "inp = inp.reshape(-1, 3)\n",
        "# add axis for preprocessing\n",
        "# from pre processing of training data\n",
        "target_len = 153\n",
        "# pre process test data\n",
        "print(\"Fixing length to 153, original length: \", str(inp.shape[0]))\n",
        "inp = sprepare_data(inp, target_len)\n",
        "print(\"New length: \", str(inp.shape[0]))\n",
        "# align\n",
        "print(\"Normalizing\")\n",
        "inp = spreprocess_point_cloud_np(inp)\n",
        "# flatten\n",
        "inp = inp.reshape((-1))\n",
        "print(\"Flattened to: \", str(inp.shape))\n",
        "# predict\n",
        "print(\"Predicting\")\n",
        "svm_model.predict(inp.reshape((1, -1)))\n",
        "print(str(np.shape(inp.reshape((1,-1)))))\n",
        "\n"
      ],
      "metadata": {
        "id": "faMt1vTAQBR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test heroku app\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Set the random seed to ensure reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Generate a random list of 579 numbers between 0 and 1.3 (inclusive)\n",
        "my_list = [random.uniform(0, 1.3) for _ in range(579)]\n",
        "\n",
        "\n",
        "# Replace this with your app's URL\n",
        "url = \"https://ar-recognition.herokuapp.com/predict\"\n",
        "\n",
        "# Replace this with your test dataset\n",
        "\"\"\"\n",
        "test_data = X_test[0].tolist()\n",
        "print(len(test_data))\n",
        "test_test = np.array(test_data)\n",
        "test_test = test_test.reshape(-1, 3)\n",
        "print(test_test.shape)\n",
        "\"\"\"\n",
        "# Send a POST request with the test dataset as a list\n",
        "response = requests.post(url, json=my_list)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    predictions = json.loads(response.text)\n",
        "    print(\"Predictions:\")\n",
        "    for data_point, prediction in zip(my_list, predictions):\n",
        "        print(f\"Input data: {data_point}, Prediction: {prediction}\")\n",
        "else:\n",
        "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
      ],
      "metadata": {
        "id": "G4PKI3hIdsgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm.predict(np.array(test_data).reshape(1, -1))\n",
        "print(sample.shape)"
      ],
      "metadata": {
        "id": "tsD-eXyHinYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}